from pathlib import Path
from joblib import Memory
import h5py
import numpy as np
from scipy.signal import butter, filtfilt, iirnotch, find_peaks, ricker, cwt
import matplotlib.pyplot as plt
from sklearn.decomposition import FastICA


DATA_DIR = Path.cwd() / 'data/' # EMG signals
CACHE_DIR = Path.cwd() / 'cache/' # cache previously done computations (via joblib)

memory = Memory(CACHE_DIR)





def butter_bandpass(lowcut, highcut, fs, order=5):
    nyq = 0.5 * fs
    b, a = butter(order, [lowcut/ nyq, highcut/ nyq], btype='band', analog=False)
    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = filtfilt(b, a, data)
    return y

def notch(notch_freq, samp_freq, quality_factor=30):
    b, a = iirnotch(notch_freq, quality_factor, samp_freq)
    return b, a

def notch_filter(data, notch_fs, fs, q=30):
    b, a = notch(notch_fs, fs, q)
    y = filtfilt(b, a, data)
    return y

@memory.cache
def filt_GRID(data, lowcut=20, highcut=500, fs=4000, order=3, notch_fs = 50, notch_q = 30):
    filt_out = np.zeros_like(data)
    for i in range(data.shape[0]):
        filt_out[i,:] = notch_filter(butter_bandpass_filter(data[i,:], lowcut, highcut, fs, order=order), notch_fs, fs, notch_q)
    return filt_out



def loadmat(filename):
    file = h5py.File(filename, 'r')
    return np.asarray(file['grid_crds']), np.asarray(file['out_mat'])

if1_grid_crds, if1_out_mat = loadmat(DATA_DIR / r'increasing-force-1.mat')
if2_grid_crds, if2_out_mat = loadmat(DATA_DIR / r'increasing-force-2.mat')
sf1_grid_crds, sf1_out_mat = loadmat(DATA_DIR / r'steady-force-1.mat')

emg = if1_out_mat

print(emg.shape)





filtered_if1_out_mat = filt_GRID(emg.T).T


def plot_emg(emg, time_bounds=None, channels=[], figsize=None):

    if not channels:
        num_rows = emg.shape[1]
        channels = range(1, emg.shape[1]+1)
    else:
        num_rows = len(channels)

    if figsize is None:
        print(num_rows)
        figsize = [10, (15 * num_rows) // 8]
        print(figsize)

    #if time_bounds is None:
    #    time_bounds = (10000, 10000+1500)

    left, right = time_bounds
    assert 0 <= left <= right

    fig, ax = plt.subplots(nrows=num_rows, figsize=figsize)
    #fig, ax = plt.subplots(nrows=1)

    for i, idx in enumerate(channels):
        curr_ax = ax[i] if len(channels) != 1 else ax # handles singleton case
        curr_ax.plot(emg[:, idx-1])
        curr_ax.set_xlim(left=left, right=right)
        curr_ax.set_ylabel(f"Electrode {idx}")
        #ax.plot(emg[:, i])
        #ax.set_xlim(left=left, right=right)
        #ax.set_ylabel(f"Electrode {i*step_size}")
    
    
    return fig, ax


#%matplotlib ipympl
channels = list(np.flipud(np.arange(64).reshape((8,8)).T + 1).ravel())
print(channels)
fig, ax = plot_emg(filtered_if1_out_mat, channels=channels, time_bounds=(1500, 5000))
plt.show()






start, stop = 0, filtered_if1_out_mat.shape[0] # start and stop time in samples
electrode = 32 # lower left electrode, closest on arm, in previously shown diagram

wavelet_convolved = []
WIDTHS = np.linspace(1, 3, 5) # roughly estimated based on total duration of Ricker wavelet, wavelet width, and durations of MUAP pulses

def muap(samples, width):
    A1 = -1
    A2 = -1
    T1 = -width
    T2 = width
    std= width
    #t = np.linspace(-4*width, 4*width, samples)
    t = np.arange(0, samples) - (samples - 1.0) / 2
    return (A1 * np.exp(-((t - T1)**2) / (2 * std**2)) - A2 * np.exp(-((t - T2)**2) / (2 * std**2)))

   # vec = np.arange(0, points) - (points - 1.0) / 2
    return total

WIDTHS = np.arange(2,10)
for i in range(emg.shape[1]):
    wavelet_convolved.append(cwt(filtered_if1_out_mat[:, i], ricker, WIDTHS))

wavelet_convolved = np.asarray(wavelet_convolved)
wavelet_convolved = np.moveaxis(wavelet_convolved, [0, 1, 2], [0, 2, 1])
#wavelet_convolved = np.sign(wavelet_convolved) * np.log(np.abs(wavelet_convolved))
#wavelet_convolved = np.exp(np.sign(wavelet_convolved) * np.abs(wavelet_convolved)**0.5)
print(wavelet_convolved.shape) # shape is electrode number, time, wavelet width


plt.figure()
for w in (ws:=[1,2,3]):
    plt.plot(muap(10*max(ws), w))
#plt.plot(muap(100, 1/2))
plt.plot(ricker(30,2))
#plt.plot(ricker(30,60))


from scipy.signal import find_peaks
from sklearn.cluster import KMeans

a = 1 # sparsity weight
b = 1 # energy weight
lambda1 = a / (1e-24 + np.linalg.norm(wavelet_convolved, axis=-1, ord=1))
#lambda2 = b * np.linalg.norm(wavelet_convolved, axis=-1, ord=np.inf)
lambda2 = b * np.max(wavelet_convolved, axis=-1)

k = 2
p = 1

#plt.figure()
#plt.plot(filtered_if1_out_mat[start:stop,electrode])

#activity = k*(lambda1 + lambda2)**p
#activity = lambda1 + lambda2 + lambda1*lambda2
#activity = lambda1*lambda2 - k*(lambda1 + lambda2)**p
activity = lambda1 * lambda2
#activity = np.exp(activity)
#activity = lambda1*lambda2/(lambda1+lambda2)
activity.shape

#plt.figure()
#plt.plot(activity[electrode,start:stop])

# NOTE: Activity must be positive everywhere (or at least at firing locations) for this to work!
s = activity[:, start:stop].shape
a_flat = activity.reshape(1,-1).T.squeeze()
p, _ = find_peaks(a_flat)

assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(np.expand_dims(a_flat[p], axis=-1))
#assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(wavelet_convolved[0,100:-100,:])

plt.figure()
plt.hist(assignments)

z = np.zeros_like(a_flat)
z[p] = assignments
assignments = z.reshape(s)
print(assignments.shape)

#assignments = assignments[electrode,:] # focus on particular electrode

unique, counts = np.unique(assignments, return_counts=True)

print(counts)

print(counts / counts.sum())


#%matplotlib widget
activity_cluster_id = np.argmin(counts)

print(activity_cluster_id)

#plt.figure()
#plt.plot(assignments == activity_cluster_id)
#plt.xlim(0,1500)

electrode = 23

plt.figure()
normalize = lambda x: (x - np.mean(x)) / np.std(x)
plt.plot(normalize(filtered_if1_out_mat[start:stop,electrode]))
pks = (assignments[electrode,:] == activity_cluster_id).astype(np.float16)
pks[pks == 0] = np.nan
plt.plot(normalize(activity[electrode,:]), label='activity', linestyle='dashed')
plt.stem(pks, 'red', label='firings',  markerfmt='x')
plt.xlim(0,2500)

fig = plt.gcf() 
fig.set_size_inches(18.5, 10.5)

plt.legend()
plt.show()


(assignments[:, start:stop] == activity_cluster_id).sum() / (assignments[:, start:stop]).size





filtered_emg = filtered_if1_out_mat.T # stack signals row-wise
filtered_emg = filtered_emg.ravel() # make it one big 1D signal

idxs, = np.where(assignments.ravel() == activity_cluster_id)
idxs = np.expand_dims(idxs, axis=0)

print(idxs)

window = 64
half_window = window // 2
idxs = idxs[idxs + half_window < filtered_emg.shape[0]]
idxs = idxs + np.expand_dims(np.arange(-half_window, half_window + 1), axis=1)

#idxs = idxs.T.ravel()
idxs = idxs.T
#mask = idxs[:,-1] < filtered_emg.shape[0]
#idxs = idxs[mask,:]


windows = filtered_emg[idxs]


windows.shape


from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)

fig, ax = plt.subplots()
fig.set_size_inches(18.5, 10.5)
fig.set_dpi(200)

# Change major ticks to show every 20.
ax.xaxis.set_major_locator(MultipleLocator(window))
ax.yaxis.set_major_locator(MultipleLocator(window))

# Change minor ticks to show every 5. (20/4 = 5)
#ax.xaxis.set_minor_locator(AutoMinorLocator(4))
#ax.yaxis.set_minor_locator(AutoMinorLocator(4))

ax.grid(which='major', color='#CCCCCC', linestyle='--')
#ax.grid(which='minor', color='#CCCCCC', linestyle=':')

plt.plot(windows[:,:].ravel())
plt.xlim(0,1500)





templates = KMeans(n_clusters=10, n_init="auto").fit(windows)

templates.cluster_centers_.shape





fig, ax = plt.subplots(nrows=len(templates.cluster_centers_))
fig.set_size_inches(1.0,10)


for i in range(len(templates.cluster_centers_)):
    ax[i].plot(templates.cluster_centers_[i,:])



from sklearn.decomposition import MiniBatchDictionaryLearning

#dict_learner = MiniBatchDictionaryLearning(n_components=15, batch_size=128, transform_algorithm='lasso_lars', transform_alpha=0.1)
#dict_learner = MiniBatchDictionaryLearning(n_components=10, batch_size=128, verbose=False, n_jobs=8, transform_algorithm='omp', alpha=200)
dict_learner = MiniBatchDictionaryLearning(n_components=10, batch_size=128, verbose=True, alpha=100, transform_algorithm='threshold')

idx = np.arange(len(windows))
np.random.shuffle(idx)
idx = idx[:1024]
X_transformed = dict_learner.fit(windows[idx,:])

dict_learner.components_


fig, ax = plt.subplots(nrows=len(dict_learner.components_))
fig.set_size_inches(1.25,10)


for i in range(len(dict_learner.components_)):
    ax[i].plot(dict_learner.components_[i,:])


activations = dict_learner.transform(windows)





plt.figure()
plt.hist((activations != 0).sum(axis=-1), bins=activations.shape[-1])


activations.sum(axis=0).shape





plt.figure()
plt.stem((activations != 0).sum(axis=0))


# plt.figure()
# plt.pcolormesh(np.abs(activations[:2000,:].T), shading='gouraud')

# plt.title('MUAP Activations')
# plt.ylabel('MUAP No.')
# plt.xlabel('Time')
# plt.gcf().set_size_inches(10,1)
# plt.show()


plt.figure()
plt.plot(windows[:,:].ravel())
plt.xlim(30*128,50*128)


s = activations.shape
assn = KMeans(n_clusters=2, n_init="auto").fit_predict((activations.reshape(1,-1).T)**1)
#assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(wavelet_convolved[0,100:-100,:])

plt.figure()
plt.hist(assn)


assn = assn.reshape(s)
print(assn.shape)

#assignments = assignments[electrode,:] # focus on particular electrode

unique, counts = np.unique(assn, return_counts=True)
cluster_of_interest = np.argmin(counts)

assn[assn == cluster_of_interest] = 1
assn[assn != cluster_of_interest] = 0

#activations = assn


peaks = np.zeros(shape=(assignments.shape[0], assignments.shape[1], assn.shape[-1]))
print(peaks.shape)

window_locs = np.vstack(np.where(assignments)).T
print(assignments.shape)

win_idxs = np.where(assignments)
win_idxs = win_idxs[0][:len(assn)], win_idxs[1][:len(assn)]

print(windows.shape)
win_idxs[0].shape

print(assn.shape)

#peaks[win_idxs[0][:-8], win_idxs[1][:-8], :] = activations[:, :] # TODO: hack for if2 out mat. replace with actual fix
peaks[win_idxs[0], win_idxs[1], :] = assn[:, :]


activations_time_sync = np.zeros(shape=(assignments.shape[0], assignments.shape[1], activations.shape[-1]))
print(peaks.shape)

#window_locs = np.vstack(np.where(assignments)).T
#print(assignments.shape)

win_idxs = np.where(assignments)
win_idxs = win_idxs[0][:len(activations)], win_idxs[1][:len(activations)]

print(windows.shape)
win_idxs[0].shape

print(assn.shape)

activations_time_sync[win_idxs[0], win_idxs[1], :] = activations[:, :]


print(activations_time_sync.shape)
plt.figure()
plt.pcolormesh(np.abs(activations_time_sync[1,...].T), shading='gouraud')

plt.title('MUAP Activations')
plt.ylabel('MUAP No.')
plt.xlabel('Time')
plt.gcf().set_size_inches(10,1)
plt.show()


plt.figure()
modified = peaks.copy()
modified[modified != 0] = 1
modified[modified != 1] = np.nan

print(modified.shape)
plt.stem(np.abs(modified[0,:,2]), 'red', markerfmt='x')
plt.xlim(250,10000)
plt.gcf().set_size_inches(20,1)



mu = 5 # motor unit
for mu in range(10):
    ft, = np.where(peaks[0,:,mu])
    isi = ft[1:] - ft[:-1]
    isi = isi[isi >= 2]
    
    print(np.mean(isi) / 2)
    isi = isi / 2 # sampling period is 0.5 ms
    plt.figure()
    n, bins, patches = plt.hist(isi, bins=100, weights=np.ones_like(isi) / isi.size)
    plt.xlabel("Firing Time (ms)")
    #plt.xlim(0,20)


n


bins




from pathlib import Path
from joblib import Memory
import h5py
import numpy as np
from scipy.signal import butter, filtfilt, iirnotch, find_peaks, ricker, cwt
import matplotlib.pyplot as plt
from sklearn.decomposition import FastICA


DATA_DIR = Path.cwd() / 'data/' # EMG signals
CACHE_DIR = Path.cwd() / 'cache/' # cache previously done computations (via joblib)

memory = Memory(CACHE_DIR)





def butter_bandpass(lowcut, highcut, fs, order=5):
    nyq = 0.5 * fs
    b, a = butter(order, [lowcut/ nyq, highcut/ nyq], btype='band', analog=False)
    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = filtfilt(b, a, data)
    return y

def notch(notch_freq, samp_freq, quality_factor=30):
    b, a = iirnotch(notch_freq, quality_factor, samp_freq)
    return b, a

def notch_filter(data, notch_fs, fs, q=30):
    b, a = notch(notch_fs, fs, q)
    y = filtfilt(b, a, data)
    return y

@memory.cache
def filt_GRID(data, lowcut=20, highcut=500, fs=4000, order=3, notch_fs = 50, notch_q = 30):
    filt_out = np.zeros_like(data)
    for i in range(data.shape[0]):
        filt_out[i,:] = notch_filter(butter_bandpass_filter(data[i,:], lowcut, highcut, fs, order=order), notch_fs, fs, notch_q)
    return filt_out



def loadmat(filename):
    file = h5py.File(filename, 'r')
    return np.asarray(file['grid_crds']), np.asarray(file['out_mat'])

if1_grid_crds, if1_out_mat = loadmat(DATA_DIR / r'increasing-force-1.mat')
if2_grid_crds, if2_out_mat = loadmat(DATA_DIR / r'increasing-force-2.mat')
sf1_grid_crds, sf1_out_mat = loadmat(DATA_DIR / r'steady-force-1.mat')

emg = sf1_out_mat

print(emg.shape)





filtered_if1_out_mat = filt_GRID(emg.T).T


def plot_emg(emg, time_bounds=None, channels=[], figsize=None):

    if not channels:
        num_rows = emg.shape[1]
        channels = range(1, emg.shape[1]+1)
    else:
        num_rows = len(channels)

    if figsize is None:
        print(num_rows)
        figsize = [10, (15 * num_rows) // 8]
        print(figsize)

    #if time_bounds is None:
    #    time_bounds = (10000, 10000+1500)

    left, right = time_bounds
    assert 0 <= left <= right

    fig, ax = plt.subplots(nrows=num_rows, figsize=figsize)
    #fig, ax = plt.subplots(nrows=1)

    for i, idx in enumerate(channels):
        curr_ax = ax[i] if len(channels) != 1 else ax # handles singleton case
        curr_ax.plot(emg[:, idx-1])
        curr_ax.set_xlim(left=left, right=right)
        curr_ax.set_ylabel(f"Electrode {idx}")
        #ax.plot(emg[:, i])
        #ax.set_xlim(left=left, right=right)
        #ax.set_ylabel(f"Electrode {i*step_size}")
    
    
    return fig, ax


#%matplotlib ipympl
channels = list(np.flipud(np.arange(64).reshape((8,8)).T + 1).ravel())
print(channels)
fig, ax = plot_emg(filtered_if1_out_mat, channels=channels, time_bounds=(1500, 5000))
plt.show()






# filtered_IC = butter_bandpass_filter(IC.T, lowcut=50, highcut=150, fs=4000).T

# fig, ax = plt.subplots(nrows=filtered_IC.shape[1]//8, figsize=[10, 10])
# for i in range(0, filtered_IC.shape[1]//8):
#     ax[i].plot(filtered_IC[:, i])
#     ax[i].set_xlim(left=10000, right=10000+1500)
# plt.show()


from sklearn.cluster import KMeans

start, stop = 0, filtered_if1_out_mat.shape[0] # start and stop time in samples
electrode = 32 # lower left electrode, closest on arm, in previously shown diagram

wavelet_convolved = []
WIDTHS = np.arange(2, 10, 1) # roughly estimated based on total duration of Ricker wavelet, wavelet width, and durations of MUAP pulses

def muap(samples, width):
    A1 = -1
    A2 = -1
    T1 = -width
    T2 = width
    std= width
    t = np.linspace(-4*width, 4*width, samples)
    return (A1 * np.exp(-((t - T1)**2) / (2 * std**2)) - A2 * np.exp(-((t - T2)**2) / (2 * std**2)))

for i in range(emg.shape[1]):
    wavelet_convolved.append(cwt(filtered_if1_out_mat[:, i], muap, WIDTHS))

wavelet_convolved = np.asarray(wavelet_convolved)
wavelet_convolved = np.moveaxis(wavelet_convolved, [0, 1, 2], [0, 2, 1])
#wavelet_convolved = np.sign(wavelet_convolved) * np.log(np.abs(wavelet_convolved))
#wavelet_convolved = np.exp(np.sign(wavelet_convolved) * np.abs(wavelet_convolved)**0.5)
print(wavelet_convolved.shape) # shape is electrode number, time, wavelet width
plt.plot(muap(100, 1))


a = 1 # sparsity weight
b = 1 # energy weight
lambda1 = a / (1e-24 + np.linalg.norm(wavelet_convolved, axis=-1, ord=1))
#lambda1 = b / np.var(wavelet_convolved, axis=-1)
#lambda2 = b * (np.linalg.norm(wavelet_convolved, axis=-1, ord=np.inf) - np.linalg.norm(wavelet_convolved, axis=-1, ord=-np.inf))
lambda2 = b * np.linalg.norm(wavelet_convolved, axis=-1, ord=-np.inf)


k = 1
p = 2
#k = 1
#p = 1

#plt.figure()
#plt.plot(filtered_if1_out_mat[start:stop,electrode])

activity = k*(lambda1 + lambda2)**p
#activity = np.exp(activity)
#activity = lambda1*lambda2/(lambda1+lambda2)
activity.shape

#plt.figure()
#plt.plot(activity[electrode,start:stop])

s = activity[:, start:stop].shape
#assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(activity[:, start:stop].reshape(1,-1).T)
cluster = KMeans(n_clusters=2, n_init="auto")
cluster = cluster.fit(activity[:, start:stop].reshape(1,-1).T)

assignments = cluster.predict(activity[:, start:stop].reshape(1,-1).T)
#assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(wavelet_convolved[0,100:-100,:])

plt.figure()
plt.hist(assignments)


assignments = assignments.reshape(s)
print(assignments.shape)

#assignments = assignments[electrode,:] # focus on particular electrode

#print(cluster.cluster_centers_)

unique, counts = np.unique(assignments, return_counts=True)
print(counts)


activity_cluster_id = np.argmin(counts)

print(activity_cluster_id)

#plt.figure()
#plt.plot(assignments == activity_cluster_id)
#plt.xlim(0,1500)

electrode = 23

plt.figure()
signal = filtered_if1_out_mat[start:stop,electrode]
plt.plot((signal - np.mean(signal)) / np.std(signal))
#ac = assignments[electrode,:] == activity_cluster_id
ac = activity[electrode,:]
plt.plot((ac - np.mean(ac)) / np.std(ac), label=p)
plt.xlim(0,1000)

fig = plt.gcf() 
fig.set_size_inches(18.5, 10.5)

plt.legend()
plt.show()


(assignments[:, start:stop] == activity_cluster_id).sum() / (assignments[:, start:stop]).size


spikes = assignments[electrode,:] == activity_cluster_id
ft, = np.where(spikes)
isi = ft[1:] - ft[:-1]
isi = isi[isi >= 2]

print(np.mean(isi) / 2)
isi = isi / 2 # sampling period is 0.5 ms
plt.figure()
plt.hist(isi, bins=100)
plt.xlabel("Firing Time (ms)")


x, = np.where((ft[1:] - ft[:-1]) <= 1)
x[0:100]


ft[x[0:100]]


spikes[2767:2767+10].astype(int)


wavelet_convolved[electrode, 2767], wavelet_convolved[electrode, 2768]


activity[electrode, 2767], activity[electrode, 2768]








filtered_emg = filtered_if1_out_mat.T # stack signals row-wise
filtered_emg = filtered_emg.ravel() # make it one big 1D signal

idxs, = np.where(assignments.ravel() == activity_cluster_id)
idxs = np.expand_dims(idxs, axis=0)

print(idxs)

window = 64
half_window = window // 2
idxs = idxs[idxs + half_window < filtered_emg.shape[0]]
idxs = idxs + np.expand_dims(np.arange(-half_window, half_window + 1), axis=1)

#idxs = idxs.T.ravel()
idxs = idxs.T
#mask = idxs[:,-1] < filtered_emg.shape[0]
#idxs = idxs[mask,:]


windows = filtered_emg[idxs]


windows.shape


from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)

fig, ax = plt.subplots()
fig.set_size_inches(18.5, 10.5)
fig.set_dpi(200)

# Change major ticks to show every 20.
ax.xaxis.set_major_locator(MultipleLocator(window))
ax.yaxis.set_major_locator(MultipleLocator(window))

# Change minor ticks to show every 5. (20/4 = 5)
#ax.xaxis.set_minor_locator(AutoMinorLocator(4))
#ax.yaxis.set_minor_locator(AutoMinorLocator(4))

ax.grid(which='major', color='#CCCCCC', linestyle='--')
#ax.grid(which='minor', color='#CCCCCC', linestyle=':')

plt.plot(windows[:,:].ravel())
plt.xlim(0,1500)





templates = KMeans(n_clusters=10, n_init="auto").fit(windows)

templates.cluster_centers_.shape





fig, ax = plt.subplots(nrows=len(templates.cluster_centers_))
fig.set_size_inches(0.75,10)


for i in range(len(templates.cluster_centers_)):
    ax[i].plot(templates.cluster_centers_[i,:])



from sklearn.decomposition import MiniBatchDictionaryLearning

#dict_learner = MiniBatchDictionaryLearning(n_components=15, batch_size=3, transform_algorithm='lasso_lars', transform_alpha=0.1)
#dict_learner = MiniBatchDictionaryLearning(n_components=10, batch_size=128, verbose=True, n_jobs=8, transform_algorithm='omp', alpha=50, fit_algorithm='cd')
dict_learner = MiniBatchDictionaryLearning(n_components=20, batch_size=128, verbose=True, alpha=100, transform_algorithm='threshold')

idx = np.arange(len(windows))
np.random.shuffle(idx)
idx = idx[:1024]
X_transformed = dict_learner.fit(windows[idx,:])

dict_learner.components_


fig, ax = plt.subplots(nrows=len(dict_learner.components_))
fig.set_size_inches(0.75,10)


for i in range(len(dict_learner.components_)):
    ax[i].plot(dict_learner.components_[i,:])


activations = dict_learner.transform(windows)


plt.hist((activations != 0).sum(axis=-1) / 10, bins=100)


activations.sum(axis=0).shape


plt.stem((activations != 0).sum(axis=0))


plt.pcolormesh(np.abs(activations[:2000,:].T), shading='gouraud')

plt.title('MUAP Activations')
plt.ylabel('MUAP No.')
plt.xlabel('Time')
plt.gcf().set_size_inches(10,1)
plt.show()


plt.plot(windows[:,:].ravel())
plt.xlim(30*128,50*128)


peaks = np.zeros(shape=(assignments.shape[0], assignments.shape[1], activations.shape[-1]))


peaks.shape


window_locs = np.vstack(np.where(assignments)).T


assignments.shape


win_idxs = np.where(assignments)
win_idxs = win_idxs[0][:len(activations)], win_idxs[1][:len(activations)]


windows.shape


win_idxs[0].shape


activations.shape


#peaks[win_idxs[0][:-8], win_idxs[1][:-8], :] = activations[:, :] # TODO: hack for if2 out mat. replace with actual fix
peaks[win_idxs[0], win_idxs[1], :] = activations[:, :]


modified = peaks.copy()
modified[modified != 0] = 1
modified[modified != 1] = np.nan
plt.stem(np.abs(modified[0,:,0]), 'red', markerfmt='x')
plt.xlim(0,8000)
plt.gcf().set_size_inches(25,1)



mu = 5 # motor unit
ft, = np.where(peaks[0,:,mu])
isi = ft[1:] - ft[:-1]
isi = isi[isi >= 2]

print(np.mean(isi) / 2)
isi = isi / 2 # sampling period is 0.5 ms
plt.figure()
plt.hist(isi, bins=100)
plt.xlabel("Firing Time (ms)")


counts, bins, _ = plt.hist(activations[:,0], bins=100)


from scipy.ndimage import binary_erosion

mu = 2
m = peaks.copy()
m[m != 0] = 1

#m = binary_erosion(m[0,:,mu], np.ones(1)).astype(float)
#m[m == 0] = np.nan

plt.stem(m[0,:,mu], 'red', markerfmt='x')
plt.xlim(0,520)
plt.gcf().set_size_inches(10,1)


mu = 2 # motor unit
ft, = np.where(peaks[0,:,mu])
isi = ft[1:] - ft[:-1]
isi = isi[isi >= 2]

print(np.mean(isi) / 2)
isi = isi / 2 # sampling period is 0.25 ms
plt.figure()
plt.hist(isi, bins=40)
plt.xlabel("Firing Time (ms)")

m[m == np.nan] = 0
ft, = np.where(m == 0)
isi = ft[1:] - ft[:-1]
isi = isi[isi >= 2]

print(np.mean(isi) / 2)
isi = isi / 2 # sampling period is 0.25 ms
plt.figure()
plt.hist(isi, bins=40)
plt.xlabel("Firing Time (ms)")


ft.shape







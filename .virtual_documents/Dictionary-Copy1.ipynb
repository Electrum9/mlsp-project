from pathlib import Path
from joblib import Memory
import h5py
import numpy as np
from scipy.signal import butter, filtfilt, iirnotch, find_peaks, ricker, cwt
import matplotlib.pyplot as plt
from sklearn.decomposition import FastICA


DATA_DIR = Path.cwd() / 'data/' # EMG signals
CACHE_DIR = Path.cwd() / 'cache/' # cache previously done computations (via joblib)

memory = Memory(CACHE_DIR)





def butter_bandpass(lowcut, highcut, fs, order=5):
    nyq = 0.5 * fs
    b, a = butter(order, [lowcut/ nyq, highcut/ nyq], btype='band', analog=False)
    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = filtfilt(b, a, data)
    return y

def notch(notch_freq, samp_freq, quality_factor=30):
    b, a = iirnotch(notch_freq, quality_factor, samp_freq)
    return b, a

def notch_filter(data, notch_fs, fs, q=30):
    b, a = notch(notch_fs, fs, q)
    y = filtfilt(b, a, data)
    return y

@memory.cache
def filt_GRID(data, lowcut=20, highcut=500, fs=4000, order=3, notch_fs = 50, notch_q = 30):
    filt_out = np.zeros_like(data)
    for i in range(data.shape[0]):
        filt_out[i,:] = notch_filter(butter_bandpass_filter(data[i,:], lowcut, highcut, fs, order=order), notch_fs, fs, notch_q)
    return filt_out



def loadmat(filename):
    file = h5py.File(filename, 'r')
    return np.asarray(file['grid_crds']), np.asarray(file['out_mat'])

if1_grid_crds, if1_out_mat = loadmat(DATA_DIR / r'increasing-force-1.mat')
if2_grid_crds, if2_out_mat = loadmat(DATA_DIR / r'increasing-force-2.mat')
sf1_grid_crds, sf1_out_mat = loadmat(DATA_DIR / r'steady-force-1.mat')

emg = if2_out_mat

print(emg.shape)





filtered_if1_out_mat = filt_GRID(emg.T).T


def plot_emg(emg, time_bounds=None, channels=[], figsize=None):

    if not channels:
        num_rows = emg.shape[1]
        channels = range(1, emg.shape[1]+1)
    else:
        num_rows = len(channels)

    if figsize is None:
        print(num_rows)
        figsize = [10, (15 * num_rows) // 8]
        print(figsize)

    #if time_bounds is None:
    #    time_bounds = (10000, 10000+1500)

    left, right = time_bounds
    assert 0 <= left <= right

    fig, ax = plt.subplots(nrows=num_rows, figsize=figsize)
    #fig, ax = plt.subplots(nrows=1)

    for i, idx in enumerate(channels):
        curr_ax = ax[i] if len(channels) != 1 else ax # handles singleton case
        curr_ax.plot(emg[:, idx-1])
        curr_ax.set_xlim(left=left, right=right)
        curr_ax.set_ylabel(f"Electrode {idx}")
        #ax.plot(emg[:, i])
        #ax.set_xlim(left=left, right=right)
        #ax.set_ylabel(f"Electrode {i*step_size}")
    
    
    return fig, ax


#%matplotlib ipympl
channels = list(np.flipud(np.arange(64).reshape((8,8)).T + 1).ravel())
print(channels)
fig, ax = plot_emg(filtered_if1_out_mat, channels=channels, time_bounds=(0, 10000))
plt.show()





# filtered_IC = butter_bandpass_filter(IC.T, lowcut=50, highcut=150, fs=4000).T

# fig, ax = plt.subplots(nrows=filtered_IC.shape[1]//8, figsize=[10, 10])
# for i in range(0, filtered_IC.shape[1]//8):
#     ax[i].plot(filtered_IC[:, i])
#     ax[i].set_xlim(left=10000, right=10000+1500)
# plt.show()


"""
from sklearn.cluster import KMeans

start, stop = 0, filtered_if1_out_mat.shape[0] # start and stop time in samples
electrode = 32 # lower left electrode, closest on arm, in previously shown diagram

wavelet_convolved = []
WIDTHS = np.arange(2, 10, 1) # roughly estimated based on total duration of Ricker wavelet, wavelet width, and durations of MUAP pulses

for i in range(emg.shape[1]):
    wavelet_convolved.append(cwt(filtered_if1_out_mat[:, i], ricker, WIDTHS))

wavelet_convolved = np.asarray(wavelet_convolved)
wavelet_convolved = np.moveaxis(wavelet_convolved, [0, 1, 2], [0, 2, 1])
print(wavelet_convolved.shape) # shape is electrode number, time, wavelet width
"""


from sklearn.cluster import KMeans

start, stop = 0, filtered_if1_out_mat.shape[0] # start and stop time in samples
electrode = 32 # lower left electrode, closest on arm, in previously shown diagram

wavelet_convolved = []
WIDTHS = np.linspace(2, 10, 10) # roughly estimated based on total duration of Ricker wavelet, wavelet width, and durations of MUAP pulses

def muap(samples, width):
    A1 = -1/width
    A2 = -1/width
    T1 = -width
    T2 = width
    std= width
    #t = np.linspace(-4*width, 4*width, samples)
    t = np.arange(0, samples) - (samples - 1.0) / 2
    return (A1 * np.exp(-((t - T1)**2) / (2 * std**2)) - A2 * np.exp(-((t - T2)**2) / (2 * std**2)))

WIDTHS = np.arange(2,10)
for i in range(emg.shape[1]):
    wavelet_convolved.append(cwt(filtered_if1_out_mat[:, i], muap, WIDTHS))

wavelet_convolved = np.asarray(wavelet_convolved)
wavelet_convolved = np.moveaxis(wavelet_convolved, [0, 1, 2], [0, 2, 1])
#wavelet_convolved = np.sign(wavelet_convolved) * np.log(np.abs(wavelet_convolved))
#wavelet_convolved = np.exp(np.sign(wavelet_convolved) * np.abs(wavelet_convolved)**0.5)
print(wavelet_convolved.shape) # shape is electrode number, time, wavelet width


## VERSION WITH PEAK DETECTION
"""
a = 1 # sparsity weight
b = 1 # energy weight
lambda1 = a / (1e-24 + np.linalg.norm(wavelet_convolved, axis=-1, ord=1))
lambda2 = b * np.linalg.norm(wavelet_convolved, axis=-1, ord=-np.inf)
#lambda2 = np.min(wavelet_convolved, axis=-1)

k = 2
p = 1

#plt.figure()
#plt.plot(filtered_if1_out_mat[start:stop,electrode])

activity = lambda1 * lambda2 - k*(lambda1 + lambda2)**p
#activity = lambda1*lambda2/(lambda1+lambda2)
activity = np.exp(activity)
#activity = lambda1*lambda2/(lambda1+lambda2)
activity.shape

# NOTE: Activity must be positive everywhere (or at least at firing locations) for this to work!
s = activity[:, start:stop].shape
a_flat = activity.reshape(1,-1).T.squeeze()
p, _ = find_peaks(a_flat)

assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(np.expand_dims(a_flat[p], axis=-1))
#assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(wavelet_convolved[0,100:-100,:])

plt.figure()
plt.hist(assignments)

z = np.zeros_like(a_flat)
z[p] = assignments
assignments = z.reshape(s)
print(assignments.shape)


assignments = assignments.reshape(s)
print(assignments.shape)

#assignments = assignments[electrode,:] # focus on particular electrode

unique, counts = np.unique(assignments, return_counts=True)
"""


a = 1 # sparsity weight
b = 1 # energy weight
lambda1 = a / (1e-24 + np.linalg.norm(wavelet_convolved, axis=-1, ord=1))
lambda2 = b * np.linalg.norm(wavelet_convolved, axis=-1, ord=-np.inf)
#lambda2 = np.min(wavelet_convolved, axis=-1)

k = 2
p = 1

#plt.figure()
#plt.plot(filtered_if1_out_mat[start:stop,electrode])

activity = lambda1 * lambda2 - k*(lambda1 + lambda2)**p
#activity = lambda1*lambda2/(lambda1+lambda2)
activity = np.exp(activity)
#activity = lambda1*lambda2/(lambda1+lambda2)
activity.shape

#plt.figure()
#plt.plot(activity[electrode,start:stop])

s = activity[:, start:stop].shape
assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(activity[:, start:stop].reshape(1,-1).T)
#assignments = KMeans(n_clusters=2, n_init="auto").fit_predict(wavelet_convolved[0,100:-100,:])

plt.figure()
plt.hist(assignments)


assignments = assignments.reshape(s)
print(assignments.shape)

#assignments = assignments[electrode,:] # focus on particular electrode

unique, counts = np.unique(assignments, return_counts=True)


activity_cluster_id = np.argmin(counts)

print(activity_cluster_id)

#plt.figure()
#plt.plot(assignments == activity_cluster_id)
#plt.xlim(0,1500)

electrode = 23

plt.figure()
plt.plot(filtered_if1_out_mat[start:stop,electrode])
plt.stem(50*(assignments[electrode,:] == activity_cluster_id), 'red', label="firings", markerfmt='x')
plt.xlim(0,5000)

fig = plt.gcf() 
fig.set_size_inches(18.5, 10.5)

plt.legend()
plt.show()


(assignments[:, start:stop] == activity_cluster_id).sum() / (assignments[:, start:stop]).size





filtered_emg = filtered_if1_out_mat.T # stack signals row-wise
filtered_emg = filtered_emg.ravel() # make it one big 1D signal

idxs, = np.where(assignments.ravel() == activity_cluster_id)
idxs = np.expand_dims(idxs, axis=0)

print(idxs)

window = 64
half_window = window // 2
idxs = idxs[idxs + half_window < filtered_emg.shape[0]]
idxs = idxs + np.expand_dims(np.arange(-half_window, half_window + 1), axis=1)

#idxs = idxs.T.ravel()
idxs = idxs.T
#mask = idxs[:,-1] < filtered_emg.shape[0]
#idxs = idxs[mask,:]


windows = filtered_emg[idxs]


windows.shape


from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)

fig, ax = plt.subplots()
fig.set_size_inches(18.5, 10.5)
fig.set_dpi(200)

# Change major ticks to show every 20.
ax.xaxis.set_major_locator(MultipleLocator(window))
ax.yaxis.set_major_locator(MultipleLocator(window))

# Change minor ticks to show every 5. (20/4 = 5)
#ax.xaxis.set_minor_locator(AutoMinorLocator(4))
#ax.yaxis.set_minor_locator(AutoMinorLocator(4))

ax.grid(which='major', color='#CCCCCC', linestyle='--')
#ax.grid(which='minor', color='#CCCCCC', linestyle=':')

plt.plot(windows[:,:].ravel())
plt.xlim(0,1500)





templates = KMeans(n_clusters=10, n_init="auto").fit(windows)

templates.cluster_centers_.shape





fig, ax = plt.subplots(ncols=len(templates.cluster_centers_)//2, nrows=2, sharex=True, sharey=True, figsize=(6, 3))
#fig.set_size_inches(10, 0.75)


for i in range(2):
    for j in range(len(templates.cluster_centers_)//2):
        ax[i,j].plot(templates.cluster_centers_[j+i*len(templates.cluster_centers_)//2,:])
        ax[i,j].set_xticks([])
        ax[i,j].set_yticks([])



from sklearn.decomposition import MiniBatchDictionaryLearning

#dict_learner = MiniBatchDictionaryLearning(n_components=15, batch_size=3, transform_algorithm='lasso_lars', transform_alpha=0.1)
#dict_learner = MiniBatchDictionaryLearning(n_components=10, batch_size=128, verbose=True, n_jobs=8, transform_algorithm='omp', alpha=50, fit_algorithm='cd')
dict_learner = MiniBatchDictionaryLearning(n_components=10, batch_size=256, verbose=True, alpha=100, transform_algorithm='threshold')

idx = np.arange(len(windows))
np.random.shuffle(idx)
idx = idx[:1024]
X_transformed = dict_learner.fit(windows[idx,:])

dict_learner.components_


fig, ax = plt.subplots(ncols=len(dict_learner.components_)//2, nrows=2, sharex=True, sharey=True, figsize=(6, 3))
#fig.set_size_inches(10, 0.75)


for i in range(2):
    for j in range(len(dict_learner.components_)//2):
        ax[i,j].plot(dict_learner.components_[j+i*len(dict_learner.components_)//2,:])
        ax[i,j].set_xticks([])
        ax[i,j].set_yticks([])



activations = dict_learner.transform(windows)


plt.hist((activations != 0).sum(axis=-1) / activations.shape[-1], bins=100)


activations.sum(axis=0).shape


plt.stem(np.mean(activations != 0, axis=0))


plt.plot(windows[:,:].ravel())
plt.xlim(30*128,50*128)


peaks = np.zeros(shape=(assignments.shape[0], assignments.shape[1], activations.shape[-1]))

window_locs = np.vstack(np.where(assignments)).T

assignments.shape

win_idxs = np.where(assignments)
win_idxs = win_idxs[0][:len(activations)], win_idxs[1][:len(activations)]

windows.shape

win_idxs[0].shape

activations.shape

#peaks[win_idxs[0][:-8], win_idxs[1][:-8], :] = activations[:, :] # TODO: hack for if2 out mat. replace with actual fix
peaks[win_idxs[0], win_idxs[1], :] = activations[:, :]

print(peaks.shape)


from scipy.ndimage import gaussian_filter

plt.pcolormesh(gaussian_filter(255*(np.abs(peaks[0,:,:].T) != 0), sigma=5), shading='gouraud')

plt.title('MUAP Activations')
plt.ylabel('MUAP No.')
plt.xlabel('Time')
plt.gcf().set_size_inches(10,1)
plt.show()


from scipy.signal import lfilter, hilbert, savgol_filter, medfilt
from scipy.ndimage import gaussian_filter1d
from obspy.signal.filter import envelope

num_firing = (peaks[0,:,:] != 0).sum(axis=-1).T

#alpha=0.9
#plt.plot(lfilter([1], [1.0, -alpha], num_firing), label='exp')
#plt.plot(gaussian_filter1d(num_firing, sigma=100), label='gaussian')
#coeffs = np.polyfit(np.arange(len(num_firing)), num_firing, 3)
#poly = np.poly1d(coeffs)
#xp = np.linspace(0, len(num_firing), 10000)
#nf_peaks, _ = find_peaks(num_firing, threshold=4)
#coeffs = np.polyfit(nf_peaks, num_firing[nf_peaks], 3)
#poly = np.poly1d(coeffs)
#xp = np.linspace(0, len(num_firing), 10000)
#smoothed = poly(xp)

#plt.plot(lfilter([1], [1.0, -0.5], num_firing))
plt.plot(num_firing, linewidth=0.75)
#plt.plot(xp, smoothed)
#plt.plot(np.convolve(num_firing, np.ones(1000)/np.sqrt(1000)), linewidth=1)
#plt.plot(np.abs(hilbert(num_firing)))
print(num_firing.shape)
plt.xlim(0,len(num_firing))
#plt.plot(num_firing)
#plt.legend()
plt.gcf().set_size_inches(20,5)


modified = peaks.copy()
modified[modified != 0] = 1
modified[modified != 1] = np.nan
plt.stem(np.abs(modified[0,:,0]), 'red', markerfmt='x')
plt.xlim(0,8000)
plt.gcf().set_size_inches(25,1)



from scipy.signal import lfilter

for mu in range(peaks.shape[-1]):
    ft, = np.where(np.abs(peaks[0,:,mu]) != 0)
    isi = ft[1:] - ft[:-1]
    isi = isi[isi >= 2]
    
    print(np.mean(isi) / 2)
    #print(len(ft[np.where((isi >= 0) & (isi <= 10))]) / len(ft)) # percentage in bad range of isi (questionable)
    print(len(ft[np.where((isi >= 10) & (isi <= 100))]) / len(ft)) # percentage in good range (typical)
    print(f"Cv = {np.std(isi)/np.mean(isi)}")
    isi = isi / 2 # sampling period is 0.5 ms
    plt.figure()
    counts, bins, patches = plt.hist(isi, bins=100, cumulative=True, weights=np.ones(len(isi))/len(isi))
    plt.xlabel("Interspike Interval Time (ms)")
    plt.xlim(0,100)

    plt.figure()
    plt.plot(lfilter(np.ones(10)/10, [1], isi)[10:])


counts, bins


print(len(ft[np.where((isi >= 0) & (isi <= 10))]) / len(ft)) # percentage in bad range of isi (questionable)
print(len(ft[np.where((isi >= 10) & (isi <= 100))]) / len(ft)) # percentage in good range (typical)


np.save("if2-firing-times.npy", (peaks[:,:,0] != 0))



